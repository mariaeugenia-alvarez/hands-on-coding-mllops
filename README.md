# üêß Masterclass Despliegue MLOps - API de Ping√ºinos

Proyecto completo de MLOps que incluye una API REST con FastAPI, procesamiento de texto con LLM, tracking con MLflow, interfaz de usuario con Streamlit y despliegue en Google Cloud Run.

## üìã Tabla de Contenidos

- [Descripci√≥n](#descripci√≥n)
- [Tecnolog√≠as](#tecnolog√≠as)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Instalaci√≥n Local](#instalaci√≥n-local)
- [Ejecuci√≥n Local](#ejecuci√≥n-local)
- [API Endpoints](#api-endpoints)
- [Interfaz de Usuario](#interfaz-de-usuario)
- [Docker](#docker)
- [Despliegue en Google Cloud Run](#despliegue-en-google-cloud-run)
- [MLflow Tracking](#mlflow-tracking)

## üìù Descripci√≥n

Este proyecto implementa una API REST completa que proporciona:
- **Gesti√≥n de datos de ping√ºinos**: Consultas y filtros sobre dataset de ping√ºinos Palmer
- **An√°lisis con ML**: Clasificaci√≥n Zero-Shot y Question Answering con transformers
- **Tracking de experimentos**: MLflow para seguimiento de experimentos con LLM
- **Interfaz de usuario**: UI interactiva construida con Streamlit
- **Despliegue cloud**: Contenedorizado y desplegado en Google Cloud Run

## üõ† Tecnolog√≠as

- **Backend**: FastAPI, Uvicorn
- **ML/AI**: Transformers (Hugging Face), LangChain, Google Gemini
- **Data**: Pandas, NumPy
- **Tracking**: MLflow
- **Containerizaci√≥n**: Docker
- **Cloud**: Google Cloud Run
- **Registry**: Docker Hub

## üìÅ Estructura del Proyecto

```
hands-on-coding-mllops/
‚îú‚îÄ‚îÄ modulos_fastapi.py          # API FastAPI con todos los endpoints
‚îú‚îÄ‚îÄ main.py                     # Script CLI para procesamiento de texto con LLM
‚îú‚îÄ‚îÄ funciones.py                # Funciones de procesamiento con MLflow tracing
‚îú‚îÄ‚îÄ ui_streamlit.py             # Interfaz de usuario con Streamlit
‚îú‚îÄ‚îÄ Dockerfile                  # Configuraci√≥n de contenedor Docker
‚îú‚îÄ‚îÄ run_ui.sh                   # Script para ejecutar UI en puerto 80
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml            # Configuraci√≥n de Streamlit
‚îú‚îÄ‚îÄ mlruns/                    # Datos de MLflow tracking
‚îú‚îÄ‚îÄ mlartifacts/               # Artefactos de MLflow
‚îî‚îÄ‚îÄ README.md                  # Este archivo
```

## üíª Instalaci√≥n Local

### Prerequisitos

- Python 3.12+
- pip
- Docker (opcional, para containerizaci√≥n)
- Google Cloud SDK (opcional, para despliegue)

### 1. Clonar el repositorio

```bash
git clone <repository-url>
cd hands-on-coding-mllops
```

### 2. Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate
```

### 3. Instalar dependencias

#### Para la API FastAPI:
```bash
pip install fastapi uvicorn pandas transformers torch sentencepiece protobuf
```

#### Para la UI Streamlit:
```bash
pip install streamlit requests pandas
```

#### Para MLflow y procesamiento LLM:
```bash
pip install mlflow langchain-google-genai python-dotenv
```

### 4. Configurar variables de entorno

Crea un archivo `.env` con tu API key de Google Gemini:

```env
GEMINI_API_KEY=tu_api_key_aqui
```

## üöÄ Ejecuci√≥n Local

### Opci√≥n 1: Ejecutar API FastAPI

```bash
uvicorn modulos_fastapi:app --reload --port 8000
```

Accede a la documentaci√≥n interactiva:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### Opci√≥n 2: Ejecutar UI de Streamlit

```bash
streamlit run ui_streamlit.py
```

Accede a la interfaz: http://localhost:8501

**Para puerto 80 con ruta /ui:**
```bash
./run_ui.sh  # Requiere sudo
```

### Opci√≥n 3: Ejecutar script de procesamiento de texto

```bash
python main.py --text "Tu texto aqu√≠" --model "gemini-2.5-flash-lite" --temperature 0.7
```

### Opci√≥n 4: MLflow UI

```bash
mlflow ui --port 5000
```

Accede al dashboard: http://localhost:5000

## üîå API Endpoints

### Ping√ºinos

#### `GET /penguins`
Filtra y lista ping√ºinos
- **Par√°metros**:
  - `sex` (opcional): "Male" o "Female"
  - `limit` (opcional): N√∫mero de resultados (default: 5)
- **Ejemplo**: `/penguins?sex=Male&limit=10`

#### `GET /penguins/{penguin_id}`
Obtiene un ping√ºino por ID
- **Par√°metro**: `penguin_id` (int)
- **Ejemplo**: `/penguins/42`

#### `GET /species`
Estad√≠sticas por especie o filtrar por especie
- **Par√°metro opcional**: `specie` ("Adelie", "Chinstrap", "Gentoo")
- **Ejemplo**: `/species?specie=Adelie`

### Machine Learning

#### `GET /zero-shot-classification`
Clasificaci√≥n de texto sin entrenamiento previo
- **Par√°metros**:
  - `text`: Texto a clasificar
  - `candidate_labels`: Etiquetas separadas por comas
- **Ejemplo**: `/zero-shot-classification?text=I love this!&candidate_labels=positive,negative,neutral`

#### `GET /question-answering`
Sistema de preguntas y respuestas basado en contexto
- **Par√°metros**:
  - `question`: Pregunta a responder
  - `context`: Contexto donde buscar la respuesta
- **Ejemplo**: `/question-answering?question=Where is it?&context=The cat is on the table`

## üé® Interfaz de Usuario

La UI de Streamlit proporciona una interfaz gr√°fica para interactuar con todos los endpoints de la API:

- ‚úÖ Filtrado interactivo de ping√ºinos
- ‚úÖ B√∫squeda por ID
- ‚úÖ Visualizaci√≥n de estad√≠sticas por especie
- ‚úÖ Clasificaci√≥n de texto Zero-Shot
- ‚úÖ Sistema de preguntas y respuestas
- ‚úÖ Gr√°ficos y visualizaciones de datos

## üê≥ Docker

### Imagen disponible en Docker Hub

La aplicaci√≥n est√° disponible en Docker Hub:
```
docker.io/mariaeu/fastapi-penguins:latest
```

### Construir imagen localmente

```bash
docker build -t fastapi-penguins .
```

### Ejecutar contenedor localmente

```bash
docker run -p 8000:8080 mariaeu/fastapi-penguins:latest
```

Accede a la API en: http://localhost:8000

### Subir imagen a Docker Hub

```bash
# Login en Docker Hub
docker login

# Etiquetar imagen
docker tag fastapi-penguins:latest tu-usuario/fastapi-penguins:latest

# Push a Docker Hub
docker push tu-usuario/fastapi-penguins:latest
```

## ‚òÅÔ∏è Despliegue en Google Cloud Run

### Prerequisitos

1. Tener instalado Google Cloud SDK:
```bash
gcloud --version
```

2. Autenticarse:
```bash
gcloud auth login
```

3. Configurar proyecto:
```bash
gcloud config set project TU_PROJECT_ID
```

### Habilitar servicios necesarios

```bash
gcloud services enable run.googleapis.com
gcloud services enable cloudbuild.googleapis.com
```

### Opci√≥n 1: Deploy desde Docker Hub

```bash
gcloud run deploy fastapi-penguins \
  --image docker.io/mariaeu/fastapi-penguins:latest \
  --platform managed \
  --region europe-west1 \
  --allow-unauthenticated \
  --port 8000 \
  --memory 2Gi \
  --cpu 2 \
  --timeout 600
```

### Opci√≥n 2: Build y Deploy desde c√≥digo fuente

```bash
gcloud run deploy fastapi-penguins \
  --source . \
  --platform managed \
  --region europe-west1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 2 \
  --timeout 600
```

### Verificar deployment

```bash
gcloud run services describe fastapi-penguins --region europe-west1
```

### Obtener URL del servicio

```bash
gcloud run services describe fastapi-penguins \
  --region europe-west1 \
  --format 'value(status.url)'
```

### Par√°metros de configuraci√≥n

- `--memory 2Gi`: Memoria asignada (necesario para modelos de transformers)
- `--cpu 2`: CPUs asignadas
- `--timeout 600`: Timeout de 10 minutos (para carga de modelos)
- `--allow-unauthenticated`: Permite acceso p√∫blico
- `--port 8000`: Puerto expuesto por la aplicaci√≥n

### Actualizar deployment

```bash
gcloud run deploy fastapi-penguins \
  --image docker.io/mariaeu/fastapi-penguins:latest \
  --platform managed \
  --region europe-west1
```

### Eliminar servicio

```bash
gcloud run services delete fastapi-penguins --region europe-west1
```

## üìä MLflow Tracking

### Iniciar servidor MLflow

```bash
mlflow server --host 0.0.0.0 --port 5000
```

### Caracter√≠sticas

- **Tracking de experimentos**: Registro autom√°tico de ejecuciones
- **Tracing**: Seguimiento detallado de llamadas a LLM
- **Artifacts**: Almacenamiento de res√∫menes y an√°lisis de sentimiento
- **M√©tricas**: Longitud de textos, scores, etc.

### Experimentos disponibles

- `Text_Analysis`: An√°lisis de texto con LLM y generaci√≥n de res√∫menes

## üîß Desarrollo

### Formato de c√≥digo

```bash
# Instalar herramientas de desarrollo
pip install black flake8 isort

# Formatear c√≥digo
black .
isort .
flake8 .
```

### Testing

```bash
# Instalar pytest
pip install pytest pytest-cov

# Ejecutar tests
pytest
```

## üìù Notas

- Los modelos de transformers se descargan autom√°ticamente en la primera ejecuci√≥n
- La primera petici√≥n a los endpoints de ML puede tardar varios segundos
- MLflow guarda los datos en `mlruns/` y `mlartifacts/`
- Para producci√≥n, considerar usar variables de entorno para configuraci√≥n sensible

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto es parte de una masterclass de MLOps.

## üë• Autores

- Tu nombre/organizaci√≥n

## üôè Agradecimientos

- Dataset de ping√ºinos Palmer de [Seaborn](https://github.com/mwaskom/seaborn-data)
- Modelos de Hugging Face
- Google Gemini API
- FastAPI y Streamlit communities

---

**Nota**: Este proyecto es con fines educativos y de demostraci√≥n de pr√°cticas de MLOps.
